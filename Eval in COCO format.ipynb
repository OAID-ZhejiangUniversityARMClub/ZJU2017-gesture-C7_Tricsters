{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import yolo.config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YoloHand(nn.Module):\n",
    "    def __init__(self, width_mul=0.125):\n",
    "        super(YoloHand, self).__init__()\n",
    "\n",
    "        self.width_mul = width_mul;\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "            )\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "            )\n",
    "        self.feature = nn.Sequential( # feature of hand\n",
    "            conv_bn(3, 10, 1),  # 3 low level preserve high res\n",
    "            conv_dw(10, int(self.width_mul* 64), 2), # 7\n",
    "            conv_dw(int(self.width_mul* 64), int(self.width_mul*64), 1), #  11\n",
    "            conv_dw(int(self.width_mul*64), int(self.width_mul*128), 2), #  19\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*128), 1), # 27\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*256), 2), # 43\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*256), 1), # 59\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*512), 2), # 91\n",
    "            \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # 133\n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # 133\n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # 133\n",
    "        )\n",
    "        \n",
    "        # transfer\n",
    "        inp = int(self.width_mul*512) \n",
    "        oup = int(self.width_mul*512)\n",
    "        self.transfer = nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(oup, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # linear\n",
    "        out_channels = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        self.final_conv = nn.Conv2d(oup, out_channels, 1, 1, padding=0, bias=True)\n",
    "    \n",
    "    def forward(self, im_data):\n",
    "        feature_map = self.feature(im_data) # get hand feature map batchsize x 320x240/8 --> 40x30\n",
    "        h = self.transfer(feature_map)\n",
    "#         h = feature_map\n",
    "        y = self.final_conv(h)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def post_process(self, y):\n",
    "        # for detection\n",
    "        bsize, c, h, w = y.size() # c = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        y_reshaped = y.permute(0, 2, 3, 1).contiguous().view(bsize, -1, cfg.num_anchors, cfg.num_classes+5) # shape=(bsize, wxh, num_a, num_c+5)\n",
    "        \n",
    "        # bbox related 0~4\n",
    "        xy_pred = F.sigmoid(y_reshaped[:, :, :, 0:2])\n",
    "        wh_pred = torch.exp(y_reshaped[:, :, :, 2:4])\n",
    "        bbox_pred = torch.cat([xy_pred, wh_pred], 3) # (bsize, wxh, num_a, 4) 4: [sig(tx), sig(ty), exp(tw), exp(th)]\n",
    "        \n",
    "        iou_pred = F.sigmoid(y_reshaped[:, :, :, 4:5]) # (bsize, wxh, num_a, 1)\n",
    "        \n",
    "        # cls related 5~end\n",
    "        score_pred = y_reshaped[:, :, :, 5:].contiguous()\n",
    "        prob_pred = F.softmax(score_pred.view(-1, score_pred.size()[-1])).view_as(score_pred) # (bsize, wxh, num_a, num_cls)\n",
    "        \n",
    "        return bbox_pred, iou_pred, prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YoloHand (\n",
       "  (feature): Sequential (\n",
       "    (0): Sequential (\n",
       "      (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (1): Sequential (\n",
       "      (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=10, bias=False)\n",
       "      (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (2): Sequential (\n",
       "      (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "      (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (3): Sequential (\n",
       "      (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=10, bias=False)\n",
       "      (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(10, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (4): Sequential (\n",
       "      (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "      (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (5): Sequential (\n",
       "      (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=20, bias=False)\n",
       "      (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(20, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (6): Sequential (\n",
       "      (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (7): Sequential (\n",
       "      (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)\n",
       "      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (8): Sequential (\n",
       "      (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "      (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (9): Sequential (\n",
       "      (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "      (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "    (10): Sequential (\n",
       "      (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "      (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): LeakyReLU (0.01, inplace)\n",
       "      (3): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): LeakyReLU (0.01, inplace)\n",
       "    )\n",
       "  )\n",
       "  (transfer): Sequential (\n",
       "    (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01, inplace)\n",
       "    (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): LeakyReLU (0.01, inplace)\n",
       "  )\n",
       "  (final_conv): Conv2d(80, 11, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = YoloHand(width_mul=0.158)\n",
    "net.load_state_dict(torch.load('models/yolohanddetect-crop-5-face-lowres-deeper-leaky-finetue'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load test COCO detection dataset\n",
    "## only five gesture classes, no face annotated as no face cls id is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "('dataset_sizes :', 500, 'class names :', {0: {u'supercategory': u'none', u'id': 0, u'name': u'five'}, 1: {u'supercategory': u'none', u'id': 1, u'name': u'l'}, 2: {u'supercategory': u'none', u'id': 2, u'name': u'one'}, 3: {u'supercategory': u'none', u'id': 3, u'name': u'seeyou'}, 4: {u'supercategory': u'none', u'id': 4, u'name': u'zero'}})\n"
     ]
    }
   ],
   "source": [
    "root = 'Datasets/coco_test/images/'\n",
    "anno_file = 'Datasets/coco_test/annotations/test.json'\n",
    "mean, std = [0.5, 0.5, 0.5],[0.25, 0.25, 0.25]\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "coco_dataset = datasets.CocoDetection(root=root, annFile=anno_file, transform = data_transform)\n",
    "\n",
    "dataset_size = len(coco_dataset)\n",
    "class_names = coco_dataset.coco.cats\n",
    "\n",
    "print('dataset_sizes :', dataset_size, 'class names :', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and store result on small crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def result_temp(image_id, category_id, bbox, score):\n",
    "    return dict(image_id=image_id, category_id=category_id, bbox=bbox, score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write result to Datasets/coco_test/result/0.75_2017-12-14-09-5-43\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from utils import postprocess\n",
    "\n",
    "use_gpu=True\n",
    "if use_gpu:\n",
    "    net.cuda()\n",
    "else:\n",
    "    net.cpu()\n",
    "    \n",
    "cfg.infer_inp_size=(192, 144)\n",
    "cfg.infer_out_size=(12, 9)\n",
    "\n",
    "result_file = []\n",
    "thresh=0.75\n",
    "for data in coco_dataset:\n",
    "    # get the inputs\n",
    "    img, target = data\n",
    "    img = img.view(1, *img.shape)\n",
    "    \n",
    "    # wrap them in Variable\n",
    "    if use_gpu:\n",
    "        img = Variable(img.cuda())\n",
    "    else:\n",
    "        img= Variable(img)\n",
    "    \n",
    "    net_output = net(img)\n",
    "    net_output = net.post_process(net_output)\n",
    "    \n",
    "    # post process\n",
    "    if use_gpu:\n",
    "        bbox_pred, iou_pred, prob_pred = [x.cpu() for x in net_output]\n",
    "    else:\n",
    "        bbox_pred, iou_pred, prob_pred = net_output\n",
    "    bbox_pred, iou_pred, prob_pred = bbox_pred.data.numpy(), iou_pred.data.numpy(), prob_pred.data.numpy()\n",
    "    post_output = postprocess(bbox_pred, iou_pred, prob_pred, cfg, thresh)\n",
    "    bboxes, scores, cls_inds = post_output\n",
    "    \n",
    "    for i in range(len(bboxes)):\n",
    "        x0, y0, x1, y1 = bboxes[i]\n",
    "        result_file.append(result_temp(\n",
    "            image_id = target[0]['image_id'],\n",
    "            category_id = cls_inds[i],\n",
    "            bbox = [x0, y0, x1-x0, y1-y0],\n",
    "            score=float(scores[i])\n",
    "        ))\n",
    "        \n",
    "result_dir = 'Datasets/coco_test/result/'\n",
    "fn = str(thresh) + '_' + time.strftime('%Y-%m-%d-%H-%-M-%S',time.localtime(time.time()))\n",
    "result_fn = os.path.join(result_dir, fn)\n",
    "print 'write result to {}'.format(result_fn)\n",
    "json.dump(result_file, open(result_fn, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.16s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.479\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.322\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.310\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.260\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.326\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.326\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.342\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.282\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO ground truth api\n",
    "dataDir='Datasets/coco_test/'\n",
    "dataType='test'\n",
    "annFile = '%s/annotations/%s.json'%(dataDir,dataType)\n",
    "cocoGt=COCO(annFile)\n",
    "\n",
    "#initialize COCO detections api\n",
    "resFile='Datasets/coco_test/result/0.75_2017-12-14-09-5-43'\n",
    "cocoDt=cocoGt.loadRes(resFile)\n",
    "\n",
    "imgIds=sorted(cocoGt.getImgIds())\n",
    "\n",
    "# running evaluation\n",
    "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
    "cocoEval.params.imgIds  = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.13s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.581\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.342\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.357\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.306\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO ground truth api\n",
    "dataDir='Datasets/coco_test/'\n",
    "dataType='test'\n",
    "annFile = '%s/annotations/%s.json'%(dataDir,dataType)\n",
    "cocoGt=COCO(annFile)\n",
    "\n",
    "#initialize COCO detections api\n",
    "resFile='Datasets/coco_test/result/0.5_2017-12-14-09-5-38'\n",
    "cocoDt=cocoGt.loadRes(resFile)\n",
    "\n",
    "imgIds=sorted(cocoGt.getImgIds())\n",
    "\n",
    "# running evaluation\n",
    "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
    "cocoEval.params.imgIds  = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.14s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.353\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.644\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.348\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.371\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.335\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.447\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO ground truth api\n",
    "dataDir='Datasets/coco_test/'\n",
    "dataType='test'\n",
    "annFile = '%s/annotations/%s.json'%(dataDir,dataType)\n",
    "cocoGt=COCO(annFile)\n",
    "\n",
    "#initialize COCO detections api\n",
    "resFile='Datasets/coco_test/result/0.25_2017-12-14-09-5-34'\n",
    "cocoDt=cocoGt.loadRes(resFile)\n",
    "\n",
    "imgIds=sorted(cocoGt.getImgIds())\n",
    "\n",
    "# running evaluation\n",
    "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
    "cocoEval.params.imgIds  = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.16s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.358\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.663\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.349\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.376\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.427\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.427\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.469\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.389\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO ground truth api\n",
    "dataDir='Datasets/coco_test/'\n",
    "dataType='test'\n",
    "annFile = '%s/annotations/%s.json'%(dataDir,dataType)\n",
    "cocoGt=COCO(annFile)\n",
    "\n",
    "#initialize COCO detections api\n",
    "resFile='Datasets/coco_test/result/0.1_2017-12-14-09-4-14'\n",
    "cocoDt=cocoGt.loadRes(resFile)\n",
    "\n",
    "imgIds=sorted(cocoGt.getImgIds())\n",
    "\n",
    "# running evaluation\n",
    "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
    "cocoEval.params.imgIds  = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
