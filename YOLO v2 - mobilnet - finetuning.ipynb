{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "from easydict import EasyDict\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith('png')\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "    return classes, class_to_idx\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for target in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def json_load(fn):\n",
    "    with open(fn) as f:\n",
    "        return EasyDict(json.load(f))\n",
    "\n",
    "    \n",
    "class MyImageFolderAndBbox(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, transform=None,  mode='train'):\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        imgs = make_dataset(root, class_to_idx)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, targets, bboxs) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "\n",
    "        hands = json_load(path[:-4]+'.json')\n",
    "        \n",
    "        detection_size=(426, 240)\n",
    "        cimg = cv2.imread(path)\n",
    "        cimg = cv2.cvtColor(cimg, cv2.COLOR_BGR2RGB)\n",
    "        block_size = (426, 240)  # 16*27  16*15\n",
    "        crop_size = (192, 144) # \n",
    "        \n",
    "        # flip\n",
    "        if random.random() >= 0.5:\n",
    "            cimg = cv2.flip(cimg, 1)\n",
    "            hands.l = flip(hands.l, block_size)\n",
    "            hands.r = flip(hands.r, block_size)\n",
    "            \n",
    "        # random scene !!!\n",
    "        while True:\n",
    "            x0 = random.randrange(0, block_size[0] - crop_size[0])\n",
    "            y0 = random.randrange(0, block_size[1] - crop_size[1])\n",
    "            if percent_of_a_in_b(hands.r, [x0,y0,x0+crop_size[0],y0+crop_size[1]]) > 0.8:\n",
    "                break\n",
    "              \n",
    "        frame = cimg[y0: y0 + crop_size[1], x0: x0 + crop_size[0]]\n",
    "        hands.l[0] -= x0\n",
    "        hands.l[2] -= x0\n",
    "        hands.r[0] -= x0\n",
    "        hands.r[2] -= x0\n",
    "        hands.l[1] -= y0\n",
    "        hands.l[3] -= y0\n",
    "        hands.r[1] -= y0\n",
    "        hands.r[3] -= y0\n",
    "        \n",
    "        gt_boxes = []\n",
    "        gt_target = []\n",
    "        # face\n",
    "        if percent_of_a_in_b(hands.l, [0,0,crop_size[0], crop_size[1]]) < 0.8:\n",
    "            x0,y0,x1,y1 = hands.l\n",
    "            if x1 > 0 and y1 > 0:\n",
    "                frame[max(0, y0): y1, max(0,x0): x1] = 255\n",
    "        else:\n",
    "            hands.l[0] = max(0, hands.l[0])\n",
    "            hands.l[1] = max(0, hands.l[1])\n",
    "            hands.l[2] = min(crop_size[0], hands.l[2])\n",
    "            hands.l[3] = min(crop_size[1], hands.l[3])\n",
    "            gt_boxes.append(hands.l)\n",
    "#             # head as extra\n",
    "            gt_target.append(len(self.classes))\n",
    "            \n",
    "        # hand\n",
    "        if percent_of_a_in_b(hands.r, [0,0,crop_size[0],crop_size[1]]) < 0.8:\n",
    "            x0,y0,x1,y1 = hands.r\n",
    "            if x1 > 0 and y1 > 0:\n",
    "                frame[max(0, y0): y1, max(0,x0): x1] = 255\n",
    "        else:\n",
    "            hands.r[0] = max(0, hands.r[0])\n",
    "            hands.r[1] = max(0, hands.r[1])\n",
    "            hands.r[2] = min(crop_size[0], hands.r[2])\n",
    "            hands.r[3] = min(crop_size[1], hands.r[3])\n",
    "            gt_boxes.append(hands.r)\n",
    "            gt_target.append(target)\n",
    "        \n",
    "        img = Image.fromarray(frame)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, np.array(gt_target), np.array(gt_boxes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "def percent_of_a_in_b(a, b):\n",
    "    ax0, ay0, ax1, ay1 = a\n",
    "    bx0, by0, bx1, by1 = b\n",
    "\n",
    "    if ax1 <= bx0 or ay1 <= by0 or ax0 >= bx1 or ay0 >= by1: # left, up, right, down\n",
    "        return 0.\n",
    "    xs = [ax0, ax1, bx0, bx1]\n",
    "    ys = [ay0, ay1, by0, by1]\n",
    "    xs = sorted(xs)\n",
    "    ys = sorted(ys)\n",
    "    # print xs, ys\n",
    "    inter_w = xs[2] - xs[1]\n",
    "    inter_h = ys[2] - ys[1]\n",
    "\n",
    "    w = ax1 - ax0\n",
    "    h = ay1 - ay0\n",
    "\n",
    "    return float(inter_w*inter_h) / (w*h)\n",
    "\n",
    "\n",
    "def flip(hand, size):\n",
    "    x0, y0, x1, y1 = hand\n",
    "    w, h = size\n",
    "    return w-x1, y0, w-x0, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def my_collate(batch):\n",
    "    \"Puts first data(images) field into a tensor with outer dimension batch size others(class label, gt box) are list\"\n",
    "    if torch.is_tensor(batch[0]):\n",
    "        out = None\n",
    "        if torch.utils.data.dataloader._use_shared_memory:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = batch[0].storage()._new_shared(numel)\n",
    "            out = batch[0].new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif type(batch[0]).__module__ == 'numpy':\n",
    "        return batch\n",
    "    elif isinstance(batch[0], collections.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [my_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError((\"batch must contain tensors, numbers, dicts or lists; found {}\"\n",
    "                     .format(type(batch[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dataset_sizes :', {'test': 250, 'val': 500}, 'class names :', ['five', 'l', 'one', 'seeyou', 'zero'])\n"
     ]
    }
   ],
   "source": [
    "DatasetDir = 'Datasets/'\n",
    "mean, std = [0.5, 0.5, 0.5],[0.25, 0.25, 0.25]\n",
    "\n",
    "data_transforms = {\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: MyImageFolderAndBbox(os.path.join(DatasetDir, x),\n",
    "                                          data_transforms[x], x)\n",
    "                  for x in ['val', 'test']}\n",
    "\n",
    "dataloders = {x: torch.utils.data.DataLoader(\n",
    "                image_datasets[x], \n",
    "                batch_size=8,                            \n",
    "                shuffle=True, \n",
    "                num_workers=4,\n",
    "                collate_fn=my_collate\n",
    ")\n",
    "              for x in ['val', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['val', 'test']}\n",
    "class_names = image_datasets['val'].classes\n",
    "\n",
    "\n",
    "print('dataset_sizes :', dataset_sizes, 'class names :', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "## keep shallow but deeper\n",
    "## leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 144)\n",
      "(12, 9)\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "import torch.nn.functional as F\n",
    "from yolo.utils.cython_bbox import bbox_ious, bbox_intersections, bbox_overlaps, anchor_intersections\n",
    "from yolo.utils.cython_yolo import yolo_to_bbox\n",
    "from multiprocessing import Pool\n",
    "from pyinn.modules import Conv2dDepthwise\n",
    "\n",
    "\n",
    "import yolo.config as cfg\n",
    "\n",
    "print cfg.inp_size\n",
    "print cfg.out_size\n",
    "\n",
    "\n",
    "class YoloHand(nn.Module):\n",
    "    def __init__(self, width_mul=0.125, use_init=False):\n",
    "        super(YoloHand, self).__init__()\n",
    "\n",
    "        self.width_mul = width_mul;\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                \n",
    "            )\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                Conv2dDepthwise(inp, 3, padding=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "            )\n",
    "        self.feature = nn.Sequential( # feature of hand\n",
    "            conv_bn(3, 10, 1),  # 3 low level preserve high res\n",
    "            conv_dw(10, int(self.width_mul* 64), 2), # 7\n",
    "            conv_dw(int(self.width_mul* 64), int(self.width_mul*64), 1), #  11\n",
    "            conv_dw(int(self.width_mul*64), int(self.width_mul*128), 2), #  19\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*128), 1), # 27\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*256), 2), # 43\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*256), 1), # 59\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*512), 2), # 91\n",
    "            \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "        )\n",
    "        \n",
    "        # transfer\n",
    "        inp = int(self.width_mul*512) \n",
    "        oup = int(self.width_mul*512)\n",
    "        self.transfer = nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(oup, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # linear\n",
    "        out_channels = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        self.final_conv = nn.Conv2d(oup, out_channels, 1, 1, padding=0, bias=True)\n",
    "        \n",
    "        # train\n",
    "        self.bbox_loss = None\n",
    "        self.iou_loss = None\n",
    "        self.cls_loss = None\n",
    "        self.pool = Pool(processes=8)\n",
    "\n",
    "    \n",
    "    def forward(self, im_data):\n",
    "        feature_map = self.feature(im_data) # get hand feature map batchsize x 320x240/8 --> 40x30\n",
    "        h = self.transfer(feature_map)\n",
    "        y = self.final_conv(h)\n",
    "        \n",
    "        # for detection\n",
    "        bsize, c, h, w = y.size() # c = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        y_reshaped = y.permute(0, 2, 3, 1).contiguous().view(bsize, -1, cfg.num_anchors, cfg.num_classes+5) # shape=(bsize, wxh, num_a, num_c+5)\n",
    "        # bbox related 0~4\n",
    "        xy_pred = F.sigmoid(y_reshaped[:, :, :, 0:2])\n",
    "        wh_pred = torch.exp(y_reshaped[:, :, :, 2:4])\n",
    "        bbox_pred = torch.cat([xy_pred, wh_pred], 3) # (bsize, wxh, num_a, 4) 4: [sig(tx), sig(ty), exp(tw), exp(th)]\n",
    "        iou_pred = F.sigmoid(y_reshaped[:, :, :, 4:5]) # (bsize, wxh, num_a, 1)\n",
    "        # cls related 5~end\n",
    "        score_pred = y_reshaped[:, :, :, 5:].contiguous()\n",
    "        prob_pred = F.softmax(score_pred.view(-1, score_pred.size()[-1])).view_as(score_pred) # (bsize, wxh, num_a, num_cls)\n",
    "        \n",
    "        return bbox_pred, iou_pred, prob_pred\n",
    "    \n",
    "    def get_loss(self, preds, gt_boxes=None, gt_classes=None, dontcare=None):\n",
    "        bbox_pred, iou_pred, prob_pred = preds\n",
    "        bbox_pred_np = bbox_pred.data.cpu().numpy()\n",
    "        iou_pred_np = iou_pred.data.cpu().numpy()\n",
    "        \n",
    "        gt_boxes_np = np.array(gt_boxes)\n",
    "        gt_classes_np = np.array(gt_classes)\n",
    "        \n",
    "        # build detection target\n",
    "        _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask = self._build_target_on_cpu(\n",
    "            bbox_pred_np, gt_boxes_np, gt_classes_np, dontcare, iou_pred_np\n",
    "        )\n",
    "\n",
    "        num_boxes = sum((len(boxes) for boxes in gt_boxes))\n",
    "        \n",
    "        box_mask = np_to_variable(_box_mask, dtype=torch.FloatTensor)\n",
    "        boxes = np_to_variable(_boxes)\n",
    "        # _boxes[:, :, :, 2:4] = torch.log(_boxes[:, :, :, 2:4])\n",
    "        box_mask = box_mask.expand_as(boxes)\n",
    "        self.bbox_loss = nn.MSELoss(size_average=False)(bbox_pred * box_mask, boxes * box_mask) / num_boxes\n",
    "        \n",
    "        iou_mask = np_to_variable(_iou_mask, dtype=torch.FloatTensor)\n",
    "        ious = np_to_variable(_ious)\n",
    "        self.iou_loss = nn.MSELoss(size_average=False)(iou_pred * iou_mask, ious * iou_mask) / num_boxes\n",
    "\n",
    "        class_mask = np_to_variable(_class_mask, dtype=torch.FloatTensor)\n",
    "        classes = np_to_variable(_classes)\n",
    "        class_mask = class_mask.expand_as(prob_pred)\n",
    "        self.cls_loss = nn.MSELoss(size_average=False)(prob_pred * class_mask, classes * class_mask) / num_boxes\n",
    "        \n",
    "        return self.bbox_loss + self.iou_loss + self.cls_loss\n",
    "    \n",
    "    def _build_target_on_cpu(self, bbox_pred_np, gt_boxes, gt_classes, dontcare, iou_pred_np):\n",
    "        \"\"\"\n",
    "        :param bbox_pred: shape: (bsize, h x w, num_anchors, 4) : (sig(tx), sig(ty), exp(tw), exp(th))\n",
    "        \"\"\"\n",
    "\n",
    "        bsize = bbox_pred_np.shape[0]\n",
    "        \n",
    "        try:\n",
    "            targets = self.pool.map(\n",
    "                _process_batch, \n",
    "                ((bbox_pred_np[b], gt_boxes[b], gt_classes[b], iou_pred_np[b]) for b in range(bsize))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.pool.close()\n",
    "            del self.pool\n",
    "            raise(e)\n",
    "\n",
    "        _boxes = np.stack(tuple((row[0] for row in targets)))\n",
    "        _ious = np.stack(tuple((row[1] for row in targets)))\n",
    "        _classes = np.stack(tuple((row[2] for row in targets)))\n",
    "        _box_mask = np.stack(tuple((row[3] for row in targets)))\n",
    "        _iou_mask = np.stack(tuple((row[4] for row in targets)))\n",
    "        _class_mask = np.stack(tuple((row[5] for row in targets)))\n",
    "\n",
    "        return _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask\n",
    "    \n",
    "def _process_batch(data):\n",
    "    bbox_pred_np, gt_boxes, gt_classes, iou_pred_np = data\n",
    "\n",
    "    # known cfg\n",
    "    W, H = cfg.out_size \n",
    "    inp_size = cfg.inp_size\n",
    "    out_size = cfg.out_size\n",
    "\n",
    "    # net output params\n",
    "    wxh, num_anchors, _ = bbox_pred_np.shape\n",
    "\n",
    "    # groud truth\n",
    "    _classes = np.zeros([wxh, num_anchors, cfg.num_classes], dtype=np.float)\n",
    "    _class_mask = np.zeros([wxh, num_anchors, 1], dtype=np.float)\n",
    "\n",
    "    _ious = np.zeros([wxh, num_anchors, 1], dtype=np.float)\n",
    "    _iou_mask = np.zeros([wxh, num_anchors, 1], dtype=np.float)\n",
    "\n",
    "    _boxes = np.zeros([wxh, num_anchors, 4], dtype=np.float)\n",
    "    _boxes[:, :, 0:2] = 0.5\n",
    "    _boxes[:, :, 2:4] = 1.0\n",
    "    _box_mask = np.zeros([wxh, num_anchors, 1], dtype=np.float) + 0.01\n",
    "\n",
    "    # scale pred_bbox\n",
    "    anchors = np.ascontiguousarray(cfg.anchors, dtype=np.float)\n",
    "    bbox_pred_np = np.expand_dims(bbox_pred_np, 0)\n",
    "    bbox_np = yolo_to_bbox(\n",
    "        np.ascontiguousarray(bbox_pred_np, dtype=np.float),\n",
    "        anchors,\n",
    "        H, W)\n",
    "    bbox_np = bbox_np[0]# bbox_np.shape (wxh, num_anchors, (x1, y1, x2, y2))   range: 0 ~ 1\n",
    "    bbox_np[:, :, 0::2] *= float(inp_size[0])  # rescale x\n",
    "    bbox_np[:, :, 1::2] *= float(inp_size[1])  # rescale y\n",
    "\n",
    "    # gt_boxes_b = np.asarray(gt_boxes[b], dtype=np.float)\n",
    "    gt_boxes = np.asarray(gt_boxes, dtype=np.float)\n",
    "\n",
    "    # for each cell, compare predicted_bbox and gt_bbox\n",
    "    bbox_np_b = np.reshape(bbox_np, [-1, 4]) # (wxhxnum_anchors, 4)\n",
    "    ious = bbox_ious(\n",
    "        np.ascontiguousarray(bbox_np_b, dtype=np.float),\n",
    "        np.ascontiguousarray(gt_boxes, dtype=np.float)\n",
    "    ) # (wxhxnum_anchors, num_gt_boxes)\n",
    "    best_ious = np.max(ious, axis=1).reshape(_iou_mask.shape)\n",
    "    iou_penalty = 0 - iou_pred_np[best_ious < cfg.iou_thresh] # no onect cell operation\n",
    "    _iou_mask[best_ious <= cfg.iou_thresh] = cfg.noobject_scale * iou_penalty # noobj mask\n",
    "\n",
    "    # locate the * cell location * of each gt_boxes\n",
    "    cell_w = float(inp_size[0]) / W # stride\n",
    "    cell_h = float(inp_size[1]) / H\n",
    "    cx = (gt_boxes[:, 0] + gt_boxes[:, 2]) * 0.5 / cell_w\n",
    "    cy = (gt_boxes[:, 1] + gt_boxes[:, 3]) * 0.5 / cell_h\n",
    "    cell_inds = np.floor(cy) * W + np.floor(cx) # !! \n",
    "    cell_inds = cell_inds.astype(np.int)\n",
    "    # get each gt_box's feature map bbox\n",
    "    target_boxes = np.empty(gt_boxes.shape, dtype=np.float)\n",
    "    target_boxes[:, 0] = cx - np.floor(cx)  # cx\n",
    "    target_boxes[:, 1] = cy - np.floor(cy)  # cy\n",
    "    target_boxes[:, 2] = (gt_boxes[:, 2] - gt_boxes[:, 0]) / inp_size[0] * out_size[0]  # tw\n",
    "    target_boxes[:, 3] = (gt_boxes[:, 3] - gt_boxes[:, 1]) / inp_size[1] * out_size[1]  # th\n",
    "\n",
    "    # for each gt boxes, match the * best match anchor type*\n",
    "    gt_boxes_resize = np.copy(gt_boxes)\n",
    "    gt_boxes_resize[:, 0::2] *= (out_size[0] / float(inp_size[0]))\n",
    "    gt_boxes_resize[:, 1::2] *= (out_size[1] / float(inp_size[1]))\n",
    "    anchor_ious = anchor_intersections(\n",
    "        anchors,\n",
    "        np.ascontiguousarray(gt_boxes_resize, dtype=np.float)\n",
    "    )\n",
    "    anchor_inds = np.argmax(anchor_ious, axis=0)\n",
    "\n",
    "    ious_reshaped = np.reshape(ious, [wxh, num_anchors, len(cell_inds)]) # len(cell_inds) == num_gt_boxes\n",
    "    for i, gt_box_cell_ind in enumerate(cell_inds):\n",
    "        if gt_box_cell_ind >= wxh or gt_box_cell_ind < 0:\n",
    "            print gt_box_cell_ind\n",
    "            continue\n",
    "\n",
    "        a = anchor_inds[i] # best match anchor index\n",
    "\n",
    "        iou_pred_of_best_anchor_cell = iou_pred_np[gt_box_cell_ind, a, :]  # 0 ~ 1, should be close to 1\n",
    "        _iou_mask[gt_box_cell_ind, a, :] = cfg.object_scale * (1 - iou_pred_of_best_anchor_cell)\n",
    "        _ious[gt_box_cell_ind, a, :] = ious_reshaped[gt_box_cell_ind, a, i]\n",
    "\n",
    "        _box_mask[gt_box_cell_ind, a, :] = cfg.coord_scale\n",
    "        target_boxes[i, 2:4] /= anchors[a]\n",
    "        _boxes[gt_box_cell_ind, a, :] = target_boxes[i]\n",
    "\n",
    "        _class_mask[gt_box_cell_ind, a, :] = cfg.class_scale\n",
    "        _classes[gt_box_cell_ind, a, gt_classes[i]] = 1.\n",
    "\n",
    "    return _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask\n",
    "    \n",
    "def np_to_variable(x, is_cuda=True, dtype=torch.FloatTensor, volatile=False):\n",
    "    v = Variable(torch.from_numpy(x).type(dtype), volatile=volatile)\n",
    "    if is_cuda:\n",
    "        v = v.cuda()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yolohandnet = YoloHand(width_mul=0.158)\n",
    "yolohandnet.load_state_dict(torch.load('models/yolohanddetect-crop-5-face-lowres-deeper-leaky-0_158-0.0187'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finetue_model(model, optimizer, scheduler, num_epochs=25):       \n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    min_loss = 2\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "#         for phase in ['val', 'test']:\n",
    "        for phase in ['test', 'val']:\n",
    "            if phase == 'test':\n",
    "                scheduler.step()\n",
    "                model.train(False)\n",
    "                model.final_conv.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            train_loss = 0.0\n",
    "            bbox_loss, iou_loss, cls_loss = 0., 0., 0.\n",
    "    \n",
    "            for data in dataloders[phase]:\n",
    "                # get the inputs\n",
    "                im, gt_classes, gt_boxes = data\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    im = Variable(im.cuda())\n",
    "                else:\n",
    "                    im = Variable(im)\n",
    "                \n",
    "                # forward\n",
    "                preds = model(im)\n",
    "\n",
    "                # loss\n",
    "                loss = model.get_loss(preds, gt_boxes, gt_classes)\n",
    "                bbox_loss += model.bbox_loss.data.cpu().numpy()[0]\n",
    "                iou_loss += model.iou_loss.data.cpu().numpy()[0]\n",
    "                cls_loss += model.cls_loss.data.cpu().numpy()[0]\n",
    "                train_loss += loss.data.cpu().numpy()[0]\n",
    "     \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'test':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            print phase\n",
    "            # analysis\n",
    "            print 'train_loss ', train_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size \n",
    "            print 'bbox_loss ', bbox_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size\n",
    "            print 'iou_loss ', iou_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size\n",
    "            print 'cls_loss ', cls_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size\n",
    "\n",
    "            # save best model\n",
    "            epoch_loss = train_loss / dataset_sizes[phase]  * dataloders[phase].batch_sampler.batch_size\n",
    "#             if phase == 'train' and epoch_loss < min_loss:\n",
    "            if phase == 'test' and epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best finetune Loss: {:4f}'.format(min_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## set dataloder \n",
    "dataloders['val'].batch_sampler.batch_size=16\n",
    "dataloders['test'].batch_sampler.batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yolohandnet.load_state_dict(torch.load('models/yolohanddetect-crop-5-face-lowres-deeper-leaky-0_158-0.0187'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "----------\n",
      "test\n",
      "train_loss  0.797084196091\n",
      "bbox_loss  0.0393536375761\n",
      "iou_loss  0.560372293949\n",
      "cls_loss  0.197358255863\n",
      "val\n",
      "train_loss  0.908167345047\n",
      "bbox_loss  0.0654243674278\n",
      "iou_loss  0.491021844625\n",
      "cls_loss  0.351721132755\n",
      "()\n",
      "Epoch 1/3\n",
      "----------\n",
      "test\n",
      "train_loss  0.420025598526\n",
      "bbox_loss  0.0322082184553\n",
      "iou_loss  0.300335801125\n",
      "cls_loss  0.0874815744162\n",
      "val\n",
      "train_loss  0.867137432098\n",
      "bbox_loss  0.0689067374468\n",
      "iou_loss  0.499203795433\n",
      "cls_loss  0.299026900768\n",
      "()\n",
      "Epoch 2/3\n",
      "----------\n",
      "test\n",
      "train_loss  0.271883547783\n",
      "bbox_loss  0.0317906343937\n",
      "iou_loss  0.163116732121\n",
      "cls_loss  0.0769761806428\n",
      "val\n",
      "train_loss  1.11156804657\n",
      "bbox_loss  0.0673407982588\n",
      "iou_loss  0.749857230425\n",
      "cls_loss  0.294370014191\n",
      "()\n",
      "Epoch 3/3\n",
      "----------\n",
      "test\n",
      "train_loss  0.25831751442\n",
      "bbox_loss  0.0272524838448\n",
      "iou_loss  0.159776792526\n",
      "cls_loss  0.0712882406116\n",
      "val\n",
      "train_loss  1.06182183456\n",
      "bbox_loss  0.073970068574\n",
      "iou_loss  0.742362075806\n",
      "cls_loss  0.245489682734\n",
      "()\n",
      "Training complete in 0m 6s\n",
      "Best finetune Loss: 0.258318\n"
     ]
    }
   ],
   "source": [
    "## train setting\n",
    "model = yolohandnet\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "import itertools    \n",
    "finetune_params = itertools.chain(model.final_conv.parameters(), model.transfer.parameters()) \n",
    "optimizer = torch.optim.Adadelta(finetune_params, lr=1) # !!! transfer learn final conv param\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "model = finetue_model(model, optimizer, step_lr_scheduler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train setting\n",
    "model = yolohandnet\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "import itertools    \n",
    "finetune_params = itertools.chain(model.final_conv.parameters(), model.transfer.parameters()) \n",
    "optimizer = torch.optim.Adadelta(finetune_params, lr=0.3) # !!! transfer learn final conv param\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "model = finetue_model(model, optimizer, step_lr_scheduler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/yolohanddetect-crop-5-face-lowres-deeper-leaky-finetue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yolohandnet.load_state_dict(torch.load('models/yolohanddetect-crop-5-face-lowres-deeper-leaky-finetue'))\n",
    "yolohandnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import deque\n",
    "from utils import postprocess, my_draw_detection\n",
    "\n",
    "cam = cv2.VideoCapture('/dev/video0')\n",
    "\n",
    "means, stds = [0.5]*3, [0.25]*3\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    yolohandnet.cuda()\n",
    "else:\n",
    "    yolohandnet.cpu()\n",
    "\n",
    "fpss = deque(maxlen=10)\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cam.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    # inference start\n",
    "    since = time.time()\n",
    "    \n",
    "    # transfrom\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, cfg.infer_inp_size)\n",
    "    timg = trans(img)\n",
    "    timg = timg.view(1, *timg.shape)\n",
    "    if use_gpu:\n",
    "        cimg = Variable(timg.cuda())\n",
    "    else:\n",
    "        cimg = Variable(timg)\n",
    "    \n",
    "    # forward\n",
    "    net_output = yolohandnet(cimg)\n",
    "    \n",
    "    # post process\n",
    "    bbox_pred, iou_pred, prob_pred = net_output\n",
    "    bbox_pred, iou_pred, prob_pred = bbox_pred.data.numpy(), iou_pred.data.numpy(), prob_pred.data.numpy()\n",
    "    post_output = postprocess(bbox_pred, iou_pred, prob_pred, cfg, 0.6)\n",
    "    bboxes, scores, cls_inds = post_output\n",
    "\n",
    "    # inference end\n",
    "    now = time.time()\n",
    "    t_frame = now - since\n",
    "    fps = 1 / t_frame\n",
    "    fpss.append(fps)\n",
    "    fps = np.mean(np.array(fpss))\n",
    "    \n",
    "    # draw rect and msg\n",
    "    frame = my_draw_detection(frame,\n",
    "                                  bboxes, scores, cls_inds,\n",
    "                                  cfg,\n",
    "                                  scale=1.0 * frame.shape[0] / img.shape[0],\n",
    "                                  thr=0,\n",
    "                                  fps=fps)\n",
    "        \n",
    "    cv2.imshow('', frame)\n",
    "    key = cv2.waitKey(1)\n",
    "        \n",
    "    if key is ord('q'):\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
