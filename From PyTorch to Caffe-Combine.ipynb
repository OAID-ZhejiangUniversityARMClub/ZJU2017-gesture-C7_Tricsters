{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine BN and conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import yolo.config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YoloHandComb(nn.Module):\n",
    "    def __init__(self, width_mul=0.125):\n",
    "        super(YoloHandComb, self).__init__()\n",
    "\n",
    "        self.width_mul = width_mul;\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                \n",
    "            )\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride=stride, padding=1, groups=inp, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "            )\n",
    "        self.feature = nn.Sequential( # feature of hand\n",
    "            conv_bn(3, 10, 1),  # 3 low level preserve high res\n",
    "            conv_dw(10, int(self.width_mul* 64), 2), # 7\n",
    "            conv_dw(int(self.width_mul* 64), int(self.width_mul*64), 1), #  11\n",
    "            conv_dw(int(self.width_mul*64), int(self.width_mul*128), 2), #  19\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*128), 1), # 27\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*256), 2), # 43\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*256), 1), # 59\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*512), 2), # 91\n",
    "            \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "        )\n",
    "        \n",
    "        # transfer\n",
    "        inp = int(self.width_mul*512) \n",
    "        oup = int(self.width_mul*512)\n",
    "        self.transfer = nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(oup, oup, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # linear\n",
    "        out_channels = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        self.final_conv = nn.Conv2d(oup, out_channels, 1, 1, padding=0, bias=True)\n",
    "        \n",
    "    def forward(self, im_data):\n",
    "        feature_map = self.feature(im_data) # get hand feature map batchsize x 320x240/8 --> 40x30\n",
    "        h = self.transfer(feature_map)\n",
    "        y = self.final_conv(h)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def post_process(self, y):\n",
    "        # for detection\n",
    "        bsize, c, h, w = y.size() # c = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        y_reshaped = y.permute(0, 2, 3, 1).contiguous().view(bsize, -1, cfg.num_anchors, cfg.num_classes+5) # shape=(bsize, wxh, num_a, num_c+5)\n",
    "        \n",
    "        # bbox related 0~4\n",
    "        xy_pred = F.sigmoid(y_reshaped[:, :, :, 0:2])\n",
    "        wh_pred = torch.exp(y_reshaped[:, :, :, 2:4])\n",
    "        bbox_pred = torch.cat([xy_pred, wh_pred], 3) # (bsize, wxh, num_a, 4) 4: [sig(tx), sig(ty), exp(tw), exp(th)]\n",
    "        \n",
    "        iou_pred = F.sigmoid(y_reshaped[:, :, :, 4:5]) # (bsize, wxh, num_a, 1)\n",
    "        \n",
    "        # cls related 5~end\n",
    "        score_pred = y_reshaped[:, :, :, 5:].contiguous()\n",
    "        prob_pred = F.softmax(score_pred.view(-1, score_pred.size()[-1])).view_as(score_pred) # (bsize, wxh, num_a, num_cls)\n",
    "        \n",
    "        return bbox_pred, iou_pred, prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YoloHand(nn.Module):\n",
    "    def __init__(self, width_mul=0.125):\n",
    "        super(YoloHand, self).__init__()\n",
    "\n",
    "        self.width_mul = width_mul;\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                \n",
    "            )\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride=stride, padding=1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "            )\n",
    "        self.feature = nn.Sequential( # feature of hand\n",
    "            conv_bn(3, 10, 1),  # 3 low level preserve high res\n",
    "            conv_dw(10, int(self.width_mul* 64), 2), # 7\n",
    "            conv_dw(int(self.width_mul* 64), int(self.width_mul*64), 1), #  11\n",
    "            conv_dw(int(self.width_mul*64), int(self.width_mul*128), 2), #  19\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*128), 1), # 27\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*256), 2), # 43\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*256), 1), # 59\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*512), 2), # 91\n",
    "            \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "        )\n",
    "        \n",
    "        # transfer\n",
    "        inp = int(self.width_mul*512) \n",
    "        oup = int(self.width_mul*512)\n",
    "        self.transfer = nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(oup, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # linear\n",
    "        out_channels = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        self.final_conv = nn.Conv2d(oup, out_channels, 1, 1, padding=0, bias=True)\n",
    "        \n",
    "    def forward(self, im_data):\n",
    "        feature_map = self.feature(im_data) # get hand feature map batchsize x 320x240/16 --> 20x15\n",
    "        h = self.transfer(feature_map)\n",
    "        y = self.final_conv(h)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def post_process(self, y):\n",
    "        # for detection\n",
    "        bsize, c, h, w = y.size() # c = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        y_reshaped = y.permute(0, 2, 3, 1).contiguous().view(bsize, -1, cfg.num_anchors, cfg.num_classes+5) # shape=(bsize, wxh, num_a, num_c+5)\n",
    "        \n",
    "        # bbox related 0~4\n",
    "        xy_pred = F.sigmoid(y_reshaped[:, :, :, 0:2])\n",
    "        wh_pred = torch.exp(y_reshaped[:, :, :, 2:4])\n",
    "        bbox_pred = torch.cat([xy_pred, wh_pred], 3) # (bsize, wxh, num_a, 4) 4: [sig(tx), sig(ty), exp(tw), exp(th)]\n",
    "        \n",
    "        iou_pred = F.sigmoid(y_reshaped[:, :, :, 4:5]) # (bsize, wxh, num_a, 1)\n",
    "        \n",
    "        # cls related 5~end\n",
    "        score_pred = y_reshaped[:, :, :, 5:].contiguous()\n",
    "        prob_pred = F.softmax(score_pred.view(-1, score_pred.size()[-1])).view_as(score_pred) # (bsize, wxh, num_a, num_cls)\n",
    "        \n",
    "        return bbox_pred, iou_pred, prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = YoloHand(width_mul=0.158)\n",
    "net.load_state_dict(torch.load('models/yolohanddetect-crop-5-face-lowres-deeper-leaky-finetue'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netc = YoloHandComb(width_mul=0.158)\n",
    "netc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first conv bn\n",
    "conv = net.feature[0][0]\n",
    "bn = net.feature[0][1]\n",
    "conv2 = netc.feature[0][0]\n",
    "\n",
    "W = conv.weight.data\n",
    "gamma = bn.weight.data \n",
    "beta = bn.bias.data\n",
    "svar = torch.sqrt(bn.running_var+bn.eps)\n",
    "mean = bn.running_mean\n",
    "\n",
    "conv2.weight.data = W * (gamma/svar).view(-1, 1,1,1)\n",
    "conv2.bias.data = beta - gamma*mean/svar\n",
    "\n",
    "# feature conv dw\n",
    "for i, layer in enumerate(net.feature):\n",
    "    if i != 0:\n",
    "        conv = net.feature[i][0]\n",
    "        bn = net.feature[i][1]\n",
    "        conv2 = netc.feature[i][0]\n",
    "        \n",
    "        W = conv.weight.data\n",
    "        gamma = bn.weight.data \n",
    "        beta = bn.bias.data\n",
    "        svar = torch.sqrt(bn.running_var+bn.eps)\n",
    "        mean = bn.running_mean\n",
    "\n",
    "        conv2.weight.data = W * (gamma/svar).view(-1, 1,1,1)\n",
    "        conv2.bias.data = beta - gamma*mean/svar\n",
    "        \n",
    "        \n",
    "        conv = net.feature[i][3]\n",
    "        bn = net.feature[i][4]\n",
    "        conv2 = netc.feature[i][2]\n",
    "        \n",
    "        W = conv.weight.data\n",
    "        gamma = bn.weight.data \n",
    "        beta = bn.bias.data\n",
    "        svar = torch.sqrt(bn.running_var+bn.eps)\n",
    "        mean = bn.running_mean\n",
    "\n",
    "        conv2.weight.data = W * (gamma/svar).view(-1, 1,1,1)\n",
    "        conv2.bias.data = beta - gamma*mean/svar\n",
    "\n",
    "# transfer\n",
    "for i, layer in enumerate(net.transfer):\n",
    "    i = i // 3\n",
    "    conv = net.transfer[i*3+0]\n",
    "    bn = net.transfer[i*3+1]\n",
    "    conv2 = netc.transfer[i*2+0]\n",
    "\n",
    "    W = conv.weight.data\n",
    "    gamma = bn.weight.data \n",
    "    beta = bn.bias.data\n",
    "    svar = torch.sqrt(bn.running_var+bn.eps)\n",
    "    mean = bn.running_mean\n",
    "\n",
    "    conv2.weight.data = W * (gamma/svar).view(-1, 1,1,1)\n",
    "    conv2.bias.data = beta - gamma*mean/svar\n",
    "\n",
    "# final conv\n",
    "netc.final_conv.weight = net.final_conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netc.state_dict(), 'models/combined-yolohanddetect-crop-5-face-lowres-deeper-leaky-finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netc.load_state_dict(torch.load('models/combined-yolohanddetect-crop-5-face-lowres-deeper-leaky-finetune'))\n",
    "netc.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice and test combine model speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from collections import deque\n",
    "from utils import postprocess, my_draw_detection\n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture('/dev/video0')\n",
    "\n",
    "means, stds = [0.5]*3, [0.25]*3\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "use_gpu = False\n",
    "yolohandnet = netc\n",
    "if use_gpu:\n",
    "    yolohandnet.cuda()\n",
    "else:\n",
    "    yolohandnet.cpu()\n",
    "\n",
    "fpss = deque(maxlen=10)\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cam.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    # inference start\n",
    "    since = time.time()\n",
    "    \n",
    "    # transfrom\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, cfg.infer_inp_size)\n",
    "    timg = trans(img)\n",
    "    timg = timg.view(1, *timg.shape)\n",
    "    if use_gpu:\n",
    "        cimg = Variable(timg.cuda())\n",
    "    else:\n",
    "        cimg = Variable(timg)\n",
    "    \n",
    "    # forward\n",
    "    net_output = yolohandnet.post_process(yolohandnet(cimg))\n",
    "    \n",
    "    # post process\n",
    "    bbox_pred, iou_pred, prob_pred = net_output\n",
    "    bbox_pred, iou_pred, prob_pred = bbox_pred.data.numpy(), iou_pred.data.numpy(), prob_pred.data.numpy()\n",
    "    post_output = postprocess(bbox_pred, iou_pred, prob_pred, cfg, 0.6)\n",
    "    bboxes, scores, cls_inds = post_output\n",
    "\n",
    "    # inference end\n",
    "    now = time.time()\n",
    "    t_frame = now - since\n",
    "    fps = 1 / t_frame\n",
    "    fpss.append(fps)\n",
    "    fps = np.mean(np.array(fpss))\n",
    "    \n",
    "    # draw rect and msg\n",
    "    frame = my_draw_detection(frame,\n",
    "                                  bboxes, scores, cls_inds,\n",
    "                                  cfg,\n",
    "                                  scale=1.0 * frame.shape[0] / img.shape[0],\n",
    "                                  thr=0,\n",
    "                                  fps=fps)\n",
    "        \n",
    "    cv2.imshow('', frame)\n",
    "    key = cv2.waitKey(1)\n",
    "        \n",
    "    if key is ord('q'):\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert to Caffe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytorch2caffe.pytorch2caffe import pytorch2caffe, plot_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = Variable(torch.rand(1, 3, 240, 320))\n",
    "output_var = netc(input_var)\n",
    "\n",
    "output_dir = 'models'\n",
    "\n",
    "# plot graph to png\n",
    "plot_graph(output_var, os.path.join(output_dir, 'yolov2.dot'))\n",
    "\n",
    "pytorch2caffe(input_var, output_var, \n",
    "              os.path.join(output_dir, 'combined-yolohanddetect-crop-5-face-lowres-deeper-leaky-finetune.prototxt'),\n",
    "              os.path.join(output_dir, 'combined-yolohanddetect-crop-5-face-lowres-deeper-leaky-finetune.caffemodel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load caffe model and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, caffe\n",
    "\n",
    "output_dir = 'models'\n",
    "model_def = os.path.join(output_dir, 'combined-yolohanddetect-crop-5-face-lowres-deeper-leaky-finetune.prototxt')\n",
    "model_weights = os.path.join(output_dir, 'combined-yolohanddetect-crop-5-face-lowres-deeper-leaky-finetune.caffemodel')\n",
    "\n",
    "caffe_net = caffe.Net(model_def,      # defines the structure of the model\n",
    "                model_weights,  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, time\n",
    "from collections import deque\n",
    "from utils import net_postprocess, postprocess, my_draw_detection\n",
    "from yolo import config as cfg\n",
    "\n",
    "cam = cv2.VideoCapture('/dev/video0')\n",
    "\n",
    "mu = np.asarray([0.5, 0.5, 0.5])\n",
    "std = np.asarray([0.25, 0.25, 0.25])\n",
    "print 'mean-subtracted values:', zip('RGB', mu)\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': caffe_net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_raw_scale('data', 1/255.)      # rescale to [0, 1] from [0, 255]\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_input_scale('data', 1/0.25)\n",
    "\n",
    "yolohandnet = caffe_net\n",
    "\n",
    "fpss = deque(maxlen=10)\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cam.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    # inference start\n",
    "    since = time.time()\n",
    "    \n",
    "    # transfrom\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, tuple(cfg.infer_inp_size))\n",
    "    transformed_image = transformer.preprocess('data', img)\n",
    "    \n",
    "    # forward\n",
    "    yolohandnet.blobs['data'].data[...] = transformed_image\n",
    "    net_output = yolohandnet.forward()['ConvNdBackward47']\n",
    "    net_output = net_postprocess(net_output, cfg)\n",
    "    \n",
    "    # post process\n",
    "    bbox_pred, iou_pred, prob_pred = net_output\n",
    "    post_output = postprocess(bbox_pred, iou_pred, prob_pred, cfg, 0.6)\n",
    "    bboxes, scores, cls_inds = post_output\n",
    "\n",
    "    # inference end\n",
    "    now = time.time()\n",
    "    t_frame = now - since\n",
    "    fps = 1 / t_frame\n",
    "    fpss.append(fps)\n",
    "    fps = np.mean(np.array(fpss))\n",
    "    \n",
    "    # draw rect and msg\n",
    "    frame = my_draw_detection(frame,\n",
    "                                  bboxes, scores, cls_inds,\n",
    "                                  cfg,\n",
    "                                  scale=1.0 * frame.shape[0] / img.shape[0],\n",
    "                                  thr=0,\n",
    "                                  fps=fps)\n",
    "        \n",
    "    cv2.imshow('', frame)\n",
    "    key = cv2.waitKey(1)\n",
    "        \n",
    "    if key is ord('q'):\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
