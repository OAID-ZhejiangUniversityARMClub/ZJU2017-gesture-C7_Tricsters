{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "from easydict import EasyDict\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith('png')\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "    return classes, class_to_idx\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for target in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def json_load(fn):\n",
    "    with open(fn) as f:\n",
    "        return EasyDict(json.load(f))\n",
    "\n",
    "    \n",
    "class MyImageFolderAndBbox(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, transform=None,  mode='train'):\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        imgs = make_dataset(root, class_to_idx)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, targets, bboxs) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "\n",
    "        hands = json_load(path[:-4]+'.json')\n",
    "        \n",
    "        detection_size=(426, 240)\n",
    "        cimg = cv2.imread(path)\n",
    "        cimg = cv2.cvtColor(cimg, cv2.COLOR_BGR2RGB)\n",
    "        # different process policy for different samples\n",
    "        if not self.classes[target] in ['seeyou', 'l'] and self.mode == 'train':\n",
    "            block_size = (284, 160) # 16*17 16*10\n",
    "            crop_size = (192, 144) # 16*12 16*9\n",
    "            scale = 1. * block_size[1] / detection_size[1]\n",
    "            cimg = cv2.resize(cimg, block_size)\n",
    "        \n",
    "            hands.l = [int(scale*z) for z in hands.l]\n",
    "            hands.r = [int(scale*z) for z in hands.r]\n",
    "        else:\n",
    "            block_size = (426, 240)  # 16*27  16*15\n",
    "            crop_size = (192, 144) # \n",
    "        \n",
    "        # flip\n",
    "        if random.random() >= 0.5:\n",
    "            cimg = cv2.flip(cimg, 1)\n",
    "            hands.l = flip(hands.l, block_size)\n",
    "            hands.r = flip(hands.r, block_size)\n",
    "            \n",
    "        # random scene\n",
    "        while True:\n",
    "            x0 = random.randrange(0, block_size[0] - crop_size[0])\n",
    "            y0 = random.randrange(0, block_size[1] - crop_size[1])\n",
    "            if percent_of_a_in_b(hands.r, [x0,y0,x0+crop_size[0],y0+crop_size[1]]) > 0.8:\n",
    "                break\n",
    "        \n",
    "        # crop\n",
    "        frame = cimg[y0: y0 + crop_size[1], x0: x0 + crop_size[0]]\n",
    "        hands.l[0] -= x0\n",
    "        hands.l[2] -= x0\n",
    "        hands.r[0] -= x0\n",
    "        hands.r[2] -= x0\n",
    "        hands.l[1] -= y0\n",
    "        hands.l[3] -= y0\n",
    "        hands.r[1] -= y0\n",
    "        hands.r[3] -= y0\n",
    "        \n",
    "        # get ground truth bboxes and targets\n",
    "        gt_boxes = []\n",
    "        gt_target = []\n",
    "        # face\n",
    "        if percent_of_a_in_b(hands.l, [0,0,crop_size[0], crop_size[1]]) < 0.8:\n",
    "            x0,y0,x1,y1 = hands.l\n",
    "            if x1 > 0 and y1 > 0:\n",
    "                frame[max(0, y0): y1, max(0,x0): x1] = 255\n",
    "        else:\n",
    "            hands.l[0] = max(0, hands.l[0])\n",
    "            hands.l[1] = max(0, hands.l[1])\n",
    "            hands.l[2] = min(crop_size[0], hands.l[2])\n",
    "            hands.l[3] = min(crop_size[1], hands.l[3])\n",
    "            gt_boxes.append(hands.l)\n",
    "            gt_target.append(len(self.classes))\n",
    "            \n",
    "        # hand\n",
    "        if percent_of_a_in_b(hands.r, [0,0,crop_size[0],crop_size[1]]) < 0.8:\n",
    "            x0,y0,x1,y1 = hands.r\n",
    "            if x1 > 0 and y1 > 0:\n",
    "                frame[max(0, y0): y1, max(0,x0): x1] = 255\n",
    "        else:\n",
    "            hands.r[0] = max(0, hands.r[0])\n",
    "            hands.r[1] = max(0, hands.r[1])\n",
    "            hands.r[2] = min(crop_size[0], hands.r[2])\n",
    "            hands.r[3] = min(crop_size[1], hands.r[3])\n",
    "            gt_boxes.append(hands.r)\n",
    "            gt_target.append(target)\n",
    "        \n",
    "        img = Image.fromarray(frame)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, np.array(gt_target), np.array(gt_boxes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "def percent_of_a_in_b(a, b):\n",
    "    ax0, ay0, ax1, ay1 = a\n",
    "    bx0, by0, bx1, by1 = b\n",
    "\n",
    "    if ax1 <= bx0 or ay1 <= by0 or ax0 >= bx1 or ay0 >= by1: # left, up, right, down\n",
    "        return 0.\n",
    "    xs = [ax0, ax1, bx0, bx1]\n",
    "    ys = [ay0, ay1, by0, by1]\n",
    "    xs = sorted(xs)\n",
    "    ys = sorted(ys)\n",
    "    inter_w = xs[2] - xs[1]\n",
    "    inter_h = ys[2] - ys[1]\n",
    "\n",
    "    w = ax1 - ax0\n",
    "    h = ay1 - ay0\n",
    "\n",
    "    return float(inter_w*inter_h) / (w*h)\n",
    "\n",
    "\n",
    "def flip(hand, size):\n",
    "    x0, y0, x1, y1 = hand\n",
    "    w, h = size\n",
    "    return w-x1, y0, w-x0, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def my_collate(batch):\n",
    "    \"Puts first data(images) field into a tensor with outer dimension batch size others(class label, gt box) are list\"\n",
    "    if torch.is_tensor(batch[0]):\n",
    "        out = None\n",
    "        if torch.utils.data.dataloader._use_shared_memory:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = batch[0].storage()._new_shared(numel)\n",
    "            out = batch[0].new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif type(batch[0]).__module__ == 'numpy':\n",
    "        return batch\n",
    "    elif isinstance(batch[0], collections.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [my_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError((\"batch must contain tensors, numbers, dicts or lists; found {}\"\n",
    "                     .format(type(batch[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transforms.ColorJitter??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import types\n",
    "Compose = transforms.Compose\n",
    "\n",
    "def adjust_brightness(img, brightness_factor):\n",
    "    \"\"\"Adjust brightness of an Image.\n",
    "\n",
    "    Args:\n",
    "        img (PIL Image): PIL Image to be adjusted.\n",
    "        brightness_factor (float):  How much to adjust the brightness. Can be\n",
    "            any non negative number. 0 gives a black image, 1 gives the\n",
    "            original image while 2 increases the brightness by a factor of 2.\n",
    "\n",
    "    Returns:\n",
    "        PIL Image: Brightness adjusted image.\n",
    "    \"\"\"\n",
    "    if not _is_pil_image(img):\n",
    "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    img = enhancer.enhance(brightness_factor)\n",
    "    return img\n",
    "\n",
    "\n",
    "def adjust_contrast(img, contrast_factor):\n",
    "    \"\"\"Adjust contrast of an Image.\n",
    "\n",
    "    Args:\n",
    "        img (PIL Image): PIL Image to be adjusted.\n",
    "        contrast_factor (float): How much to adjust the contrast. Can be any\n",
    "            non negative number. 0 gives a solid gray image, 1 gives the\n",
    "            original image while 2 increases the contrast by a factor of 2.\n",
    "\n",
    "    Returns:\n",
    "        PIL Image: Contrast adjusted image.\n",
    "    \"\"\"\n",
    "    if not _is_pil_image(img):\n",
    "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
    "\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(contrast_factor)\n",
    "    return img\n",
    "\n",
    "class ColorJitter(object):\n",
    "    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n",
    "\n",
    "    Args:\n",
    "        brightness (float): How much to jitter brightness. brightness_factor\n",
    "            is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n",
    "        contrast (float): How much to jitter contrast. contrast_factor\n",
    "            is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n",
    "        saturation (float): How much to jitter saturation. saturation_factor\n",
    "            is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n",
    "        hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n",
    "            [-hue, hue]. Should be >=0 and <= 0.5.\n",
    "    \"\"\"\n",
    "    def __init__(self, brightness=0, contrast=0):\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(brightness, contrast):\n",
    "        \"\"\"Get a randomized transform to be applied on image.\n",
    "\n",
    "        Arguments are same as that of __init__.\n",
    "\n",
    "        Returns:\n",
    "            Transform which randomly adjusts brightness, contrast and\n",
    "            saturation in a random order.\n",
    "        \"\"\"\n",
    "        transforms = []\n",
    "        if brightness > 0:\n",
    "            brightness_factor = np.random.uniform(1, 1 + brightness)\n",
    "            transforms.append(Lambda(lambda img: adjust_brightness(img, brightness_factor)))\n",
    "\n",
    "        if contrast > 0:\n",
    "            contrast_factor = np.random.uniform(1, 1 + contrast)\n",
    "            transforms.append(Lambda(lambda img: adjust_contrast(img, contrast_factor)))\n",
    "\n",
    "        np.random.shuffle(transforms)\n",
    "        transform = Compose(transforms)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Input image.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Color jittered image.\n",
    "        \"\"\"\n",
    "        transform = self.get_params(self.brightness, self.contrast)\n",
    "        return transform(img)\n",
    "\n",
    "class Lambda(object):\n",
    "    \"\"\"Apply a user-defined lambda as a transform.\n",
    "\n",
    "    Args:\n",
    "        lambd (function): Lambda/function to be used for transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambd):\n",
    "        assert isinstance(lambd, types.LambdaType)\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.lambd(img)\n",
    "\n",
    "def _is_pil_image(img):\n",
    "    if accimage is not None:\n",
    "        return isinstance(img, (Image.Image, accimage.Image))\n",
    "    else:\n",
    "        return isinstance(img, Image.Image)\n",
    "\n",
    "\n",
    "def _is_tensor_image(img):\n",
    "    return torch.is_tensor(img) and img.ndimension() == 3\n",
    "\n",
    "\n",
    "def _is_numpy_image(img):\n",
    "    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dataset_sizes :', {'train': 9000, 'val': 250}, 'class names :', ['five', 'l', 'one', 'seeyou', 'zero'])\n"
     ]
    }
   ],
   "source": [
    "DatasetDir = 'Datasets/'\n",
    "mean, std = [0.5, 0.5, 0.5],[0.25, 0.25, 0.25]\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        ColorJitter(brightness=0.3, contrast=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: MyImageFolderAndBbox(os.path.join(DatasetDir, x),\n",
    "                                          data_transforms[x], x)\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "dataloders = {x: torch.utils.data.DataLoader(\n",
    "                image_datasets[x], \n",
    "                batch_size=8,                            \n",
    "                shuffle=True, \n",
    "                num_workers=4,\n",
    "                collate_fn=my_collate\n",
    ")\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "print('dataset_sizes :', dataset_sizes, 'class names :', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       " -0.3843 -0.3686 -0.9961  ...   0.9490  1.0275  1.0902\n",
       " -0.1961 -0.2118  0.1490  ...  -0.2431 -0.0706 -0.2118\n",
       " -0.9647 -0.9333 -0.9961  ...  -0.4784 -0.4157 -0.3686\n",
       "           ...             ⋱             ...          \n",
       "  0.8549  0.3059  1.0275  ...   1.0588  0.5255  0.8549\n",
       "  1.0902  1.0118  0.6824  ...   1.0118  1.0275  0.1961\n",
       "  0.5882  0.6824  0.5882  ...   0.9176  0.7765  0.8392\n",
       "\n",
       "( 1 ,.,.) = \n",
       " -0.2118 -0.2118 -0.8549  ...   1.3412  1.3882  1.4824\n",
       " -0.0235 -0.0235  0.3529  ...   0.0078  0.2118  0.1020\n",
       " -0.8078 -0.7451 -0.8706  ...  -0.2275 -0.1647 -0.1020\n",
       "           ...             ⋱             ...          \n",
       "  0.7765  0.2588  0.9490  ...   1.0118  0.4471  0.7765\n",
       "  1.0118  0.9333  0.5882  ...   0.9333  0.9490  0.1176\n",
       "  0.4471  0.5412  0.4471  ...   0.8549  0.7294  0.7765\n",
       "\n",
       "( 2 ,.,.) = \n",
       " -1.6863 -1.5608 -2.0000  ...   1.6235  1.7176  1.7961\n",
       " -1.4824 -1.4510 -1.0588  ...   0.0078  0.2118  0.1020\n",
       " -2.0000 -2.0000 -2.0000  ...  -0.2902 -0.2275 -0.1333\n",
       "           ...             ⋱             ...          \n",
       " -0.7137 -1.2314 -0.5255  ...  -0.4471 -0.9333 -0.6039\n",
       " -0.4471 -0.5412 -0.8706  ...  -0.4784 -0.4000 -1.2784\n",
       " -0.9961 -0.9020 -0.9647  ...  -0.5569 -0.6510 -0.6039\n",
       "[torch.FloatTensor of size 3x144x192]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'][1000][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " ( 0 , 0 ,.,.) = \n",
       "   0.6039  0.5725  0.5725  ...   1.2157  1.2471  1.2627\n",
       "   0.6039  0.5725  0.5569  ...   1.2471  1.2627  1.2784\n",
       "   0.6039  0.6039  0.5725  ...   1.2627  1.2784  1.3098\n",
       "            ...             ⋱             ...          \n",
       "   1.1373  1.1373  1.1529  ...  -0.0549  0.1647  0.2118\n",
       "   1.0902  1.1059  1.0902  ...  -0.1333  0.1647  0.1961\n",
       "   1.1373  1.1686  1.1686  ...  -0.3373  0.1176  0.1804\n",
       " \n",
       " ( 0 , 1 ,.,.) = \n",
       "   0.6667  0.6353  0.6353  ...   1.3412  1.3255  1.3412\n",
       "   0.6667  0.6353  0.6196  ...   1.3569  1.3725  1.4039\n",
       "   0.6667  0.6667  0.6353  ...   1.4039  1.4353  1.4510\n",
       "            ...             ⋱             ...          \n",
       "   1.1843  1.2157  1.1843  ...  -0.0392  0.1804  0.2275\n",
       "   1.1529  1.1843  1.1686  ...  -0.0549  0.1961  0.2118\n",
       "   1.1843  1.2627  1.2627  ...  -0.2588  0.1804  0.2118\n",
       " \n",
       " ( 0 , 2 ,.,.) = \n",
       "   0.5255  0.5098  0.5098  ...   1.2157  1.2627  1.2627\n",
       "   0.5255  0.5098  0.4784  ...   1.2471  1.2784  1.3098\n",
       "   0.5255  0.5255  0.5098  ...   1.2784  1.3098  1.3255\n",
       "            ...             ⋱             ...          \n",
       "   1.0902  1.0745  1.0902  ...  -0.1647  0.0235  0.1020\n",
       "   1.0745  1.0902  1.1059  ...  -0.2118  0.0078  0.0235\n",
       "   1.1373  1.1529  1.1843  ...  -0.4471  0.0078  0.0235\n",
       "       ⋮  \n",
       " \n",
       " ( 1 , 0 ,.,.) = \n",
       "   0.5255  0.4941  0.4941  ...   1.3412  1.3098  1.3098\n",
       "   0.5412  0.5255  0.5255  ...   1.3569  1.3569  1.3569\n",
       "   0.5725  0.5255  0.5412  ...   1.3725  1.3725  1.3725\n",
       "            ...             ⋱             ...          \n",
       "  -1.0118 -1.5137 -1.3882  ...  -1.3412 -1.3412 -1.3255\n",
       "  -1.0902 -1.5294 -1.3098  ...  -1.3412 -1.3412 -1.3255\n",
       "  -1.0745 -1.2941 -1.3882  ...  -1.3412 -1.3098 -1.3098\n",
       " \n",
       " ( 1 , 1 ,.,.) = \n",
       "   0.5725  0.5725  0.5725  ...   1.4196  1.4039  1.4039\n",
       "   0.5882  0.5882  0.5882  ...   1.4353  1.4353  1.4353\n",
       "   0.6039  0.5882  0.6039  ...   1.4667  1.4667  1.4667\n",
       "            ...             ⋱             ...          \n",
       "  -1.0588 -1.5608 -1.4196  ...  -1.3098 -1.3098 -1.2941\n",
       "  -1.0745 -1.4510 -1.2157  ...  -1.3098 -1.3098 -1.2941\n",
       "  -1.1216 -1.2627 -1.2784  ...  -1.3098 -1.2784 -1.2941\n",
       " \n",
       " ( 1 , 2 ,.,.) = \n",
       "   0.4157  0.4157  0.4157  ...   1.3569  1.3412  1.3412\n",
       "   0.4471  0.4471  0.4471  ...   1.3725  1.3725  1.3725\n",
       "   0.4627  0.4471  0.4627  ...   1.4039  1.4039  1.4039\n",
       "            ...             ⋱             ...          \n",
       "  -1.2627 -1.7333 -1.6392  ...  -1.4039 -1.4039 -1.4039\n",
       "  -1.2627 -1.6078 -1.3882  ...  -1.3412 -1.3882 -1.4039\n",
       "  -1.2784 -1.4196 -1.4353  ...  -1.4039 -1.3412 -1.3882\n",
       "       ⋮  \n",
       " \n",
       " ( 2 , 0 ,.,.) = \n",
       "  -0.9647 -1.0118 -1.0431  ...  -0.2118 -0.2275 -0.2902\n",
       "   2.0000  2.0000  2.0000  ...  -0.3216 -0.3529 -0.3843\n",
       "   2.0000  2.0000  2.0000  ...  -0.4000 -0.4784 -0.4784\n",
       "            ...             ⋱             ...          \n",
       "  -0.8392 -0.7765 -0.7765  ...  -1.9059 -2.0000 -1.9373\n",
       "  -0.8392 -0.7922 -0.7765  ...  -1.9373 -2.0000 -2.0000\n",
       "  -0.8392 -0.8706 -0.7608  ...  -2.0000 -2.0000 -2.0000\n",
       " \n",
       " ( 2 , 1 ,.,.) = \n",
       "  -0.9961 -1.0275 -1.0745  ...  -0.2902 -0.3059 -0.3373\n",
       "   2.0000  2.0000  2.0000  ...  -0.3529 -0.4000 -0.4157\n",
       "   2.0000  2.0000  2.0000  ...  -0.4471 -0.5255 -0.5255\n",
       "            ...             ⋱             ...          \n",
       "  -0.7765 -0.6824 -0.6824  ...  -1.8902 -1.9843 -1.9216\n",
       "  -0.7765 -0.6824 -0.6667  ...  -1.9216 -2.0000 -2.0000\n",
       "  -0.7765 -0.7765 -0.6510  ...  -2.0000 -2.0000 -2.0000\n",
       " \n",
       " ( 2 , 2 ,.,.) = \n",
       "  -1.1529 -1.2314 -1.2627  ...  -0.4471 -0.4627 -0.5098\n",
       "   2.0000  2.0000  2.0000  ...  -0.5882 -0.6353 -0.6510\n",
       "   2.0000  2.0000  2.0000  ...  -0.6667 -0.7608 -0.7608\n",
       "            ...             ⋱             ...          \n",
       "  -0.8706 -0.7922 -0.7922  ...  -2.0000 -2.0000 -2.0000\n",
       "  -0.8706 -0.8235 -0.8235  ...  -2.0000 -2.0000 -2.0000\n",
       "  -0.8706 -0.9176 -0.8078  ...  -2.0000 -2.0000 -2.0000\n",
       " ...     \n",
       "       ⋮  \n",
       " \n",
       " ( 5 , 0 ,.,.) = \n",
       "  -1.6549 -0.5882 -0.5882  ...   1.1059 -0.1333  1.0902\n",
       "  -1.6863 -1.3725 -0.5098  ...   1.9216  0.9333  1.2471\n",
       "  -1.3412 -0.9333 -1.3098  ...  -0.0863  1.3412  1.2941\n",
       "            ...             ⋱             ...          \n",
       "   2.0000  2.0000  2.0000  ...   2.0000  2.0000  2.0000\n",
       "   2.0000  2.0000  2.0000  ...   2.0000  2.0000  1.8275\n",
       "   2.0000  2.0000  1.8745  ...   2.0000  1.8745  2.0000\n",
       " \n",
       " ( 5 , 1 ,.,.) = \n",
       "  -1.3255 -0.2431 -0.2275  ...   1.5451  0.3373  1.5451\n",
       "  -1.3725 -1.0118 -0.1333  ...   2.0000  1.3725  1.7020\n",
       "  -1.0588 -0.5725 -0.9176  ...   0.3529  1.7961  1.7176\n",
       "            ...             ⋱             ...          \n",
       "   2.0000  2.0000  2.0000  ...   2.0000  2.0000  2.0000\n",
       "   2.0000  2.0000  2.0000  ...   2.0000  2.0000  1.9529\n",
       "   2.0000  2.0000  1.9686  ...   2.0000  2.0000  2.0000\n",
       " \n",
       " ( 5 , 2 ,.,.) = \n",
       "  -1.8431 -0.9961 -1.1686  ...   0.4941 -0.7608  0.2745\n",
       "  -1.8431 -1.6549 -0.8863  ...   1.3255  0.2588  0.4471\n",
       "  -1.4667 -1.0588 -1.4824  ...  -0.6353  0.6980  0.5882\n",
       "            ...             ⋱             ...          \n",
       "   0.5255  0.7137  0.9647  ...   0.6039  0.2431  0.4471\n",
       "   0.8706  1.1216  0.8392  ...   0.1020  0.1647 -0.1333\n",
       "   0.5412  0.4000  0.2431  ...   0.3373 -0.1020  0.5255\n",
       "       ⋮  \n",
       " \n",
       " ( 6 , 0 ,.,.) = \n",
       "   1.8275  1.8588  1.8275  ...   2.0000  1.1059  0.8078\n",
       "   1.8275  1.8275  1.8118  ...   1.9843  0.5569  0.8706\n",
       "   1.8588  1.8588  1.8275  ...   2.0000  0.6039  0.9020\n",
       "            ...             ⋱             ...          \n",
       "  -0.3216 -0.2902 -0.0235  ...  -1.8431 -1.8431 -1.8745\n",
       "  -0.1961  0.0549  0.0078  ...  -1.8745 -1.8431 -1.8745\n",
       "  -1.2941 -1.4196 -1.5137  ...  -1.8902 -1.8745 -1.7490\n",
       " \n",
       " ( 6 , 1 ,.,.) = \n",
       "   1.8118  1.8275  1.8118  ...   2.0000  1.1373  0.8549\n",
       "   1.8118  1.8118  1.7961  ...   2.0000  0.5882  0.9020\n",
       "   1.8275  1.8275  1.8118  ...   2.0000  0.6510  0.9490\n",
       "            ...             ⋱             ...          \n",
       "  -0.4000 -0.3529 -0.0549  ...  -1.7176 -1.7176 -1.7490\n",
       "  -0.2902 -0.0078 -0.0078  ...  -1.7490 -1.7176 -1.7490\n",
       "  -1.3569 -1.4353 -1.5294  ...  -1.7647 -1.7490 -1.6392\n",
       " \n",
       " ( 6 , 2 ,.,.) = \n",
       "   1.6706  1.7020  1.6706  ...   1.8275  0.8078  0.5725\n",
       "   1.6706  1.6706  1.6549  ...   1.7020  0.2588  0.6196\n",
       "   1.7020  1.7020  1.6706  ...   1.8902  0.3529  0.6667\n",
       "            ...             ⋱             ...          \n",
       "  -0.5725 -0.5412 -0.2118  ...  -1.6863 -1.6863 -1.7333\n",
       "  -0.4627 -0.1961 -0.1804  ...  -1.7333 -1.6863 -1.7333\n",
       "  -1.5608 -1.6392 -1.6863  ...  -1.7490 -1.7333 -1.6235\n",
       "       ⋮  \n",
       " \n",
       " ( 7 , 0 ,.,.) = \n",
       "   1.5765  1.6078  1.6078  ...   2.0000  2.0000  2.0000\n",
       "   1.5765  1.6078  1.6078  ...   2.0000  2.0000  2.0000\n",
       "   1.5765  1.6078  1.6078  ...   2.0000  2.0000  2.0000\n",
       "            ...             ⋱             ...          \n",
       "  -0.1490 -0.0863 -0.0706  ...   0.1333  0.1176  0.0392\n",
       "  -0.0235 -0.0235 -0.0235  ...   0.2745  0.2745  0.2275\n",
       "  -0.0549 -0.0235  0.0235  ...   0.3059  0.2745  0.2431\n",
       " \n",
       " ( 7 , 1 ,.,.) = \n",
       "   1.7176  1.7333  1.7333  ...   2.0000  2.0000  2.0000\n",
       "   1.7176  1.7333  1.7333  ...   2.0000  2.0000  2.0000\n",
       "   1.7176  1.7333  1.7333  ...   2.0000  2.0000  2.0000\n",
       "            ...             ⋱             ...          \n",
       "  -0.7294 -0.6353 -0.6196  ...  -0.3529 -0.3686 -0.4000\n",
       "  -0.6196 -0.6196 -0.6196  ...  -0.2745 -0.2902 -0.3216\n",
       "  -0.6353 -0.6196 -0.5882  ...  -0.2745 -0.3529 -0.3686\n",
       " \n",
       " ( 7 , 2 ,.,.) = \n",
       "   1.7176  1.7333  1.7333  ...   2.0000  2.0000  2.0000\n",
       "   1.7176  1.7333  1.7333  ...   2.0000  2.0000  2.0000\n",
       "   1.7176  1.7333  1.7333  ...   2.0000  2.0000  2.0000\n",
       "            ...             ⋱             ...          \n",
       "  -1.4980 -1.4353 -1.4353  ...  -1.0588 -1.0118 -1.0588\n",
       "  -1.4196 -1.4196 -1.4510  ...  -1.0275 -0.9961 -1.0118\n",
       "  -1.4667 -1.4510 -1.4196  ...  -1.0588 -1.0588 -1.0588\n",
       " [torch.FloatTensor of size 8x3x144x192],\n",
       " (array([5, 2]),\n",
       "  array([5, 0]),\n",
       "  array([5, 1]),\n",
       "  array([5, 1]),\n",
       "  array([5, 2]),\n",
       "  array([5, 0]),\n",
       "  array([1]),\n",
       "  array([5, 0])),\n",
       " (array([[ 79,  50, 119,  89],\n",
       "         [149,  60, 175, 100]]), array([[ 36,  40,  71,  75],\n",
       "         [ 81,  37, 115,  74]]), array([[ 49,  61,  99, 111],\n",
       "         [134,  65, 172, 109]]), array([[ 99,  71, 149, 121],\n",
       "         [160,  50, 192,  89]]), array([[ 73,  29, 113,  70],\n",
       "         [  9,  38,  35,  84]]), array([[ 81,  39, 119,  77],\n",
       "         [ 17,  39,  55,  80]]), array([[133,  60, 167,  93]]), array([[ 64,  13, 108,  58],\n",
       "         [131,  13, 169,  61]]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloders['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "## keep shallow but deeper\n",
    "## leaky ReLU\n",
    "## modified from: https://github.com/longcw/yolo2-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 144)\n",
      "(12, 9)\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "import torch.nn.functional as F\n",
    "from yolo.utils.cython_bbox import bbox_ious, bbox_intersections, bbox_overlaps, anchor_intersections\n",
    "from yolo.utils.cython_yolo import yolo_to_bbox\n",
    "from multiprocessing import Pool\n",
    "from pyinn.modules import Conv2dDepthwise\n",
    "\n",
    "\n",
    "import yolo.config as cfg\n",
    "print cfg.inp_size\n",
    "print cfg.out_size\n",
    "\n",
    "\n",
    "class YoloHand(nn.Module):\n",
    "    def __init__(self, width_mul=0.125, use_init=False):\n",
    "        super(YoloHand, self).__init__()\n",
    "\n",
    "        self.width_mul = width_mul;\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                \n",
    "            )\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                Conv2dDepthwise(inp, 3, padding=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "            )\n",
    "        self.feature = nn.Sequential( # feature of hand\n",
    "            conv_bn(3, 10, 1),  # 3 low level preserve high res\n",
    "            conv_dw(10, int(self.width_mul* 64), 2), # 7\n",
    "            conv_dw(int(self.width_mul* 64), int(self.width_mul*64), 1), #  11\n",
    "            conv_dw(int(self.width_mul*64), int(self.width_mul*128), 2), #  19\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*128), 1), # 27\n",
    "            conv_dw(int(self.width_mul*128), int(self.width_mul*256), 2), # 43\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*256), 1), # 59\n",
    "            conv_dw(int(self.width_mul*256), int(self.width_mul*512), 2), # 91\n",
    "            \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # 133\n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), # \n",
    "            conv_dw(int(self.width_mul*512), int(self.width_mul*512), 1), #\n",
    "        )\n",
    "        \n",
    "        # transfer\n",
    "        inp = int(self.width_mul*512) \n",
    "        oup = int(self.width_mul*512)\n",
    "        self.transfer = nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(oup, oup, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # linear\n",
    "        out_channels = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        self.final_conv = nn.Conv2d(oup, out_channels, 1, 1, padding=0, bias=True)\n",
    "        \n",
    "        # train\n",
    "        self.bbox_loss = None\n",
    "        self.iou_loss = None\n",
    "        self.cls_loss = None\n",
    "        self.pool = Pool(processes=8)\n",
    "\n",
    "    def load_feature_weights(self, pretrained_model):\n",
    "        sd = torch.load(pretrained_model)\n",
    "        from collections import OrderedDict\n",
    "        sd1 = OrderedDict()\n",
    "        for key in self.feature.state_dict().keys():\n",
    "            sd1[key] = sd['feature.'+key]\n",
    "        self.feature.load_state_dict(sd1)\n",
    "    \n",
    "    def forward(self, im_data):\n",
    "        feature_map = self.feature(im_data) # get hand feature map \n",
    "        h = self.transfer(feature_map)\n",
    "        y = self.final_conv(h)\n",
    "        \n",
    "        # for detection\n",
    "        bsize, c, h, w = y.size() # c = cfg.num_anchors * (cfg.num_classes + 5)\n",
    "        y_reshaped = y.permute(0, 2, 3, 1).contiguous().view(bsize, -1, cfg.num_anchors, cfg.num_classes+5) # shape=(bsize, wxh, num_a, num_c+5)\n",
    "        # bbox related 0~4\n",
    "        xy_pred = F.sigmoid(y_reshaped[:, :, :, 0:2])\n",
    "        wh_pred = torch.exp(y_reshaped[:, :, :, 2:4])\n",
    "        bbox_pred = torch.cat([xy_pred, wh_pred], 3) # (bsize, wxh, num_a, 4) 4: [sig(tx), sig(ty), exp(tw), exp(th)]\n",
    "        iou_pred = F.sigmoid(y_reshaped[:, :, :, 4:5]) # (bsize, wxh, num_a, 1)\n",
    "        # cls related 5~end\n",
    "        score_pred = y_reshaped[:, :, :, 5:].contiguous()\n",
    "        prob_pred = F.softmax(score_pred.view(-1, score_pred.size()[-1])).view_as(score_pred) # (bsize, wxh, num_a, num_cls)\n",
    "        \n",
    "        return bbox_pred, iou_pred, prob_pred\n",
    "    \n",
    "    def get_loss(self, preds, gt_boxes=None, gt_classes=None, dontcare=None):\n",
    "        bbox_pred, iou_pred, prob_pred = preds\n",
    "        bbox_pred_np = bbox_pred.data.cpu().numpy()\n",
    "        iou_pred_np = iou_pred.data.cpu().numpy()\n",
    "        \n",
    "        gt_boxes_np = np.array(gt_boxes)\n",
    "        gt_classes_np = np.array(gt_classes)\n",
    "        \n",
    "        # build detection target\n",
    "        _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask = self._build_target_on_cpu(\n",
    "            bbox_pred_np, gt_boxes_np, gt_classes_np, dontcare, iou_pred_np\n",
    "        )\n",
    "\n",
    "        num_boxes = sum((len(boxes) for boxes in gt_boxes))\n",
    "        \n",
    "        box_mask = np_to_variable(_box_mask, dtype=torch.FloatTensor)\n",
    "        boxes = np_to_variable(_boxes)\n",
    "        # _boxes[:, :, :, 2:4] = torch.log(_boxes[:, :, :, 2:4])\n",
    "        box_mask = box_mask.expand_as(boxes)\n",
    "        self.bbox_loss = nn.MSELoss(size_average=False)(bbox_pred * box_mask, boxes * box_mask) / num_boxes\n",
    "        \n",
    "        iou_mask = np_to_variable(_iou_mask, dtype=torch.FloatTensor)\n",
    "        ious = np_to_variable(_ious)\n",
    "        self.iou_loss = nn.MSELoss(size_average=False)(iou_pred * iou_mask, ious * iou_mask) / num_boxes\n",
    "\n",
    "        class_mask = np_to_variable(_class_mask, dtype=torch.FloatTensor)\n",
    "        classes = np_to_variable(_classes)\n",
    "        class_mask = class_mask.expand_as(prob_pred)\n",
    "        self.cls_loss = nn.MSELoss(size_average=False)(prob_pred * class_mask, classes * class_mask) / num_boxes\n",
    "        \n",
    "        return self.bbox_loss + self.iou_loss + self.cls_loss\n",
    "    \n",
    "    def _build_target_on_cpu(self, bbox_pred_np, gt_boxes, gt_classes, dontcare, iou_pred_np):\n",
    "        \"\"\"\n",
    "        :param bbox_pred: shape: (bsize, h x w, num_anchors, 4) : (sig(tx), sig(ty), exp(tw), exp(th))\n",
    "        \"\"\"\n",
    "\n",
    "        bsize = bbox_pred_np.shape[0]\n",
    "        \n",
    "        try:\n",
    "            targets = self.pool.map(\n",
    "                _process_batch, \n",
    "                ((bbox_pred_np[b], gt_boxes[b], gt_classes[b], iou_pred_np[b]) for b in range(bsize))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.pool.close()\n",
    "            del self.pool\n",
    "            raise(e)\n",
    "\n",
    "        _boxes = np.stack(tuple((row[0] for row in targets)))\n",
    "        _ious = np.stack(tuple((row[1] for row in targets)))\n",
    "        _classes = np.stack(tuple((row[2] for row in targets)))\n",
    "        _box_mask = np.stack(tuple((row[3] for row in targets)))\n",
    "        _iou_mask = np.stack(tuple((row[4] for row in targets)))\n",
    "        _class_mask = np.stack(tuple((row[5] for row in targets)))\n",
    "\n",
    "        return _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask\n",
    "    \n",
    "def _process_batch(data):\n",
    "    bbox_pred_np, gt_boxes, gt_classes, iou_pred_np = data\n",
    "\n",
    "    # known cfg\n",
    "    W, H = cfg.out_size \n",
    "    inp_size = cfg.inp_size\n",
    "    out_size = cfg.out_size\n",
    "\n",
    "    # net output params\n",
    "    wxh, num_anchors, _ = bbox_pred_np.shape\n",
    "\n",
    "    # groud truth\n",
    "    _classes = np.zeros([wxh, num_anchors, cfg.num_classes], dtype=np.float)\n",
    "    _class_mask = np.zeros([wxh, num_anchors, 1], dtype=np.float)\n",
    "\n",
    "    _ious = np.zeros([wxh, num_anchors, 1], dtype=np.float)\n",
    "    _iou_mask = np.zeros([wxh, num_anchors, 1], dtype=np.float)\n",
    "\n",
    "    _boxes = np.zeros([wxh, num_anchors, 4], dtype=np.float)\n",
    "    _boxes[:, :, 0:2] = 0.5\n",
    "    _boxes[:, :, 2:4] = 1.0\n",
    "    _box_mask = np.zeros([wxh, num_anchors, 1], dtype=np.float) + 0.01\n",
    "\n",
    "    # scale pred_bbox\n",
    "    anchors = np.ascontiguousarray(cfg.anchors, dtype=np.float)\n",
    "    bbox_pred_np = np.expand_dims(bbox_pred_np, 0)\n",
    "    bbox_np = yolo_to_bbox(\n",
    "        np.ascontiguousarray(bbox_pred_np, dtype=np.float),\n",
    "        anchors,\n",
    "        H, W)\n",
    "    bbox_np = bbox_np[0]# bbox_np.shape (wxh, num_anchors, (x1, y1, x2, y2))   range: 0 ~ 1\n",
    "    bbox_np[:, :, 0::2] *= float(inp_size[0])  # rescale x\n",
    "    bbox_np[:, :, 1::2] *= float(inp_size[1])  # rescale y\n",
    "\n",
    "    # gt_boxes_b = np.asarray(gt_boxes[b], dtype=np.float)\n",
    "    gt_boxes = np.asarray(gt_boxes, dtype=np.float)\n",
    "\n",
    "    # for each cell, compare predicted_bbox and gt_bbox\n",
    "    bbox_np_b = np.reshape(bbox_np, [-1, 4]) # (wxhxnum_anchors, 4)\n",
    "    ious = bbox_ious(\n",
    "        np.ascontiguousarray(bbox_np_b, dtype=np.float),\n",
    "        np.ascontiguousarray(gt_boxes, dtype=np.float)\n",
    "    ) # (wxhxnum_anchors, num_gt_boxes)\n",
    "    best_ious = np.max(ious, axis=1).reshape(_iou_mask.shape)\n",
    "    iou_penalty = 0 - iou_pred_np[best_ious < cfg.iou_thresh] # no onect cell operation\n",
    "    _iou_mask[best_ious <= cfg.iou_thresh] = cfg.noobject_scale * iou_penalty # noobj mask\n",
    "\n",
    "    # locate the * cell location * of each gt_boxes\n",
    "    cell_w = float(inp_size[0]) / W # stride\n",
    "    cell_h = float(inp_size[1]) / H\n",
    "    cx = (gt_boxes[:, 0] + gt_boxes[:, 2]) * 0.5 / cell_w\n",
    "    cy = (gt_boxes[:, 1] + gt_boxes[:, 3]) * 0.5 / cell_h\n",
    "    cell_inds = np.floor(cy) * W + np.floor(cx) # !! \n",
    "    cell_inds = cell_inds.astype(np.int)\n",
    "    # get each gt_box's feature map bbox\n",
    "    target_boxes = np.empty(gt_boxes.shape, dtype=np.float)\n",
    "    target_boxes[:, 0] = cx - np.floor(cx)  # cx\n",
    "    target_boxes[:, 1] = cy - np.floor(cy)  # cy\n",
    "    target_boxes[:, 2] = (gt_boxes[:, 2] - gt_boxes[:, 0]) / inp_size[0] * out_size[0]  # tw\n",
    "    target_boxes[:, 3] = (gt_boxes[:, 3] - gt_boxes[:, 1]) / inp_size[1] * out_size[1]  # th\n",
    "\n",
    "    # for each gt boxes, match the * best match anchor type*\n",
    "    gt_boxes_resize = np.copy(gt_boxes)\n",
    "    gt_boxes_resize[:, 0::2] *= (out_size[0] / float(inp_size[0]))\n",
    "    gt_boxes_resize[:, 1::2] *= (out_size[1] / float(inp_size[1]))\n",
    "    anchor_ious = anchor_intersections(\n",
    "        anchors,\n",
    "        np.ascontiguousarray(gt_boxes_resize, dtype=np.float)\n",
    "    )\n",
    "    anchor_inds = np.argmax(anchor_ious, axis=0)\n",
    "\n",
    "    ious_reshaped = np.reshape(ious, [wxh, num_anchors, len(cell_inds)]) # len(cell_inds) == num_gt_boxes\n",
    "    for i, gt_box_cell_ind in enumerate(cell_inds):\n",
    "        if gt_box_cell_ind >= wxh or gt_box_cell_ind < 0:\n",
    "            print gt_box_cell_ind\n",
    "            continue\n",
    "\n",
    "        a = anchor_inds[i] # best match anchor index\n",
    "\n",
    "        iou_pred_of_best_anchor_cell = iou_pred_np[gt_box_cell_ind, a, :]  # 0 ~ 1, should be close to 1\n",
    "        _iou_mask[gt_box_cell_ind, a, :] = cfg.object_scale * (1 - iou_pred_of_best_anchor_cell)\n",
    "        _ious[gt_box_cell_ind, a, :] = ious_reshaped[gt_box_cell_ind, a, i]\n",
    "\n",
    "        _box_mask[gt_box_cell_ind, a, :] = cfg.coord_scale\n",
    "        target_boxes[i, 2:4] /= anchors[a]\n",
    "        _boxes[gt_box_cell_ind, a, :] = target_boxes[i]\n",
    "\n",
    "        _class_mask[gt_box_cell_ind, a, :] = cfg.class_scale\n",
    "        _classes[gt_box_cell_ind, a, gt_classes[i]] = 1.\n",
    "\n",
    "    return _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask\n",
    "    \n",
    "def np_to_variable(x, is_cuda=True, dtype=torch.FloatTensor, volatile=False):\n",
    "    v = Variable(torch.from_numpy(x).type(dtype), volatile=volatile)\n",
    "    if is_cuda:\n",
    "        v = v.cuda()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs=25):       \n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    min_loss = 2\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            train_loss = 0.0\n",
    "            bbox_loss, iou_loss, cls_loss = 0., 0., 0.\n",
    "    \n",
    "            for data in dataloders[phase]:\n",
    "                # get the inputs\n",
    "                im, gt_classes, gt_boxes = data\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    im = Variable(im.cuda())\n",
    "                else:\n",
    "                    im = Variable(im)\n",
    "                \n",
    "                # forward\n",
    "                preds = model(im)\n",
    "\n",
    "                # loss\n",
    "                loss = model.get_loss(preds, gt_boxes, gt_classes)\n",
    "                bbox_loss += model.bbox_loss.data.cpu().numpy()[0]\n",
    "                iou_loss += model.iou_loss.data.cpu().numpy()[0]\n",
    "                cls_loss += model.cls_loss.data.cpu().numpy()[0]\n",
    "                train_loss += loss.data.cpu().numpy()[0]\n",
    "     \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            print phase\n",
    "            # analysis\n",
    "            print 'train_loss ', train_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size \n",
    "            print 'bbox_loss ', bbox_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size\n",
    "            print 'iou_loss ', iou_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size\n",
    "            print 'cls_loss ', cls_loss / dataset_sizes[phase] * dataloders[phase].batch_sampler.batch_size\n",
    "\n",
    "            # save best model\n",
    "            epoch_loss = train_loss / dataset_sizes[phase]  * dataloders[phase].batch_sampler.batch_size\n",
    "            if phase == 'val' and epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best train Loss: {:4f}'.format(min_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set dataloder \n",
    "dataloders['train'].batch_sampler.batch_size=16\n",
    "dataloders['val'].batch_sampler.batch_size=16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define model load pretrained feature extractor\n",
    "yolohandnet = YoloHand(width_mul=0.158)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolohandnet.load_feature_weights('models/feature-val-0.94')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.933733261771\n",
      "bbox_loss  0.110295577245\n",
      "iou_loss  0.464519727561\n",
      "cls_loss  0.358917956594\n",
      "val\n",
      "train_loss  2.93074257278\n",
      "bbox_loss  0.250947208881\n",
      "iou_loss  2.04116367722\n",
      "cls_loss  0.638631709099\n",
      "()\n",
      "Epoch 1/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.40149415544\n",
      "bbox_loss  0.059838491612\n",
      "iou_loss  0.203645204069\n",
      "cls_loss  0.13801045986\n",
      "val\n",
      "train_loss  2.71939309692\n",
      "bbox_loss  0.239399947166\n",
      "iou_loss  1.95972803116\n",
      "cls_loss  0.520265093803\n",
      "()\n",
      "Epoch 2/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.287430665506\n",
      "bbox_loss  0.0501019831796\n",
      "iou_loss  0.155871294811\n",
      "cls_loss  0.081457387198\n",
      "val\n",
      "train_loss  2.67591493225\n",
      "bbox_loss  0.245864940643\n",
      "iou_loss  1.85895313644\n",
      "cls_loss  0.571096877098\n",
      "()\n",
      "Epoch 3/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.242034714295\n",
      "bbox_loss  0.0440639249219\n",
      "iou_loss  0.137896161574\n",
      "cls_loss  0.0600746279705\n",
      "val\n",
      "train_loss  2.54749996948\n",
      "bbox_loss  0.280367138863\n",
      "iou_loss  1.76078181648\n",
      "cls_loss  0.506351026535\n",
      "()\n",
      "Epoch 4/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.212399018639\n",
      "bbox_loss  0.0402746438823\n",
      "iou_loss  0.125744237206\n",
      "cls_loss  0.0463801374248\n",
      "val\n",
      "train_loss  2.22618669891\n",
      "bbox_loss  0.225440544128\n",
      "iou_loss  1.51171874332\n",
      "cls_loss  0.489027425766\n",
      "()\n",
      "Epoch 5/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.183733585583\n",
      "bbox_loss  0.0382224423372\n",
      "iou_loss  0.105861664888\n",
      "cls_loss  0.0396494780361\n",
      "val\n",
      "train_loss  1.98735220718\n",
      "bbox_loss  0.199936825752\n",
      "iou_loss  1.28487180138\n",
      "cls_loss  0.502543597221\n",
      "()\n",
      "Epoch 6/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.16059315945\n",
      "bbox_loss  0.0356581770107\n",
      "iou_loss  0.0943349575284\n",
      "cls_loss  0.0306000248358\n",
      "val\n",
      "train_loss  1.93415182495\n",
      "bbox_loss  0.207464092731\n",
      "iou_loss  1.25160648346\n",
      "cls_loss  0.475081251144\n",
      "()\n",
      "Epoch 7/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.146238312407\n",
      "bbox_loss  0.034001016231\n",
      "iou_loss  0.0846718663085\n",
      "cls_loss  0.0275654297797\n",
      "val\n",
      "train_loss  2.34613993454\n",
      "bbox_loss  0.228839742661\n",
      "iou_loss  1.61779947472\n",
      "cls_loss  0.499500731468\n",
      "()\n",
      "Epoch 8/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.135890286057\n",
      "bbox_loss  0.0324922993316\n",
      "iou_loss  0.0780863061605\n",
      "cls_loss  0.0253116806412\n",
      "val\n",
      "train_loss  2.0749958725\n",
      "bbox_loss  0.173331650972\n",
      "iou_loss  1.43874170113\n",
      "cls_loss  0.46292251873\n",
      "()\n",
      "Epoch 9/9\n",
      "----------\n",
      "train\n",
      "train_loss  0.125404247882\n",
      "bbox_loss  0.0309097198811\n",
      "iou_loss  0.0733413949658\n",
      "cls_loss  0.021153132979\n",
      "val\n",
      "train_loss  1.79950026321\n",
      "bbox_loss  0.174506494522\n",
      "iou_loss  1.10685180855\n",
      "cls_loss  0.518141962528\n",
      "()\n",
      "Training complete in 3m 47s\n",
      "Best train Loss: 1.799500\n"
     ]
    }
   ],
   "source": [
    "## train setting\n",
    "model = yolohandnet\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "        \n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-2)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer, step_lr_scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train setting\n",
    "model = yolohandnet\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.3, weigh)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.3)\n",
    "\n",
    "model = train_model(model, optimizer, step_lr_scheduler, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train setting\n",
    "model = yolohandnet\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.3)\n",
    "\n",
    "model = train_model(model, optimizer, step_lr_scheduler, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/yolohanddetect-crop-5-face-lowres-deeper-leaky-0_158-0.0187')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yolohandnet = YoloHand(width_mul=0.158)\n",
    "yolohandnet.load_state_dict(torch.load('models/yolohanddetect-crop-5-face-lowres-deeper-leaky-0_158-0.0187'))\n",
    "yolohandnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fully conv\n",
    "import cv2\n",
    "from collections import deque\n",
    "from utils import postprocess, my_draw_detection\n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture('/dev/video0')\n",
    "\n",
    "means, stds = [0.5]*3, [0.25]*3\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    yolohandnet.cuda()\n",
    "else:\n",
    "    yolohandnet.cpu()\n",
    "\n",
    "fpss = deque(maxlen=10)\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cam.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    # inference start\n",
    "    since = time.time()\n",
    "    \n",
    "    # transfrom\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, cfg.infer_inp_size)\n",
    "    timg = trans(img)\n",
    "    timg = timg.view(1, *timg.shape)\n",
    "    if use_gpu:\n",
    "        cimg = Variable(timg.cuda())\n",
    "    else:\n",
    "        cimg = Variable(timg)\n",
    "    \n",
    "    # forward\n",
    "    net_output = yolohandnet(cimg)\n",
    "    \n",
    "    # post process\n",
    "    bbox_pred, iou_pred, prob_pred = net_output\n",
    "    bbox_pred, iou_pred, prob_pred = bbox_pred.data.numpy(), iou_pred.data.numpy(), prob_pred.data.numpy()\n",
    "    post_output = postprocess(bbox_pred, iou_pred, prob_pred, cfg, 0.6)\n",
    "    bboxes, scores, cls_inds = post_output\n",
    "\n",
    "    # inference end\n",
    "    now = time.time()\n",
    "    t_frame = now - since\n",
    "    fps = 1 / t_frame\n",
    "    fpss.append(fps)\n",
    "    fps = np.mean(np.array(fpss))\n",
    "    \n",
    "    # draw rect and msg\n",
    "    frame = my_draw_detection(frame,\n",
    "                                  bboxes, scores, cls_inds,\n",
    "                                  cfg,\n",
    "                                  scale=1.0 * frame.shape[0] / img.shape[0],\n",
    "                                  thr=0,\n",
    "                                  fps=fps)\n",
    "        \n",
    "    cv2.imshow('', frame)\n",
    "    key = cv2.waitKey(1)\n",
    "        \n",
    "    if key is ord('q'):\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
